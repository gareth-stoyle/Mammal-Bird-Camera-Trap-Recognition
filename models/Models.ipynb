{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5681d1d7-b379-4748-aaaf-614cb42acae8",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2eef1-b0ef-42ef-8d66-b1385c1587b1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437bab47-3eb9-4db4-afb1-49d719ffec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f5455-951f-4fb8-bd9d-b06be076bd9a",
   "metadata": {},
   "source": [
    "### Load Model and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09315c7e-cb99-4b70-adef-a0feda0bc310",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"ConvNextBase\"  # \"EfficientNetV2M\"  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09cf9797-0094-47d1-938e-9dd698280d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNextBase already downloaded...\n"
     ]
    }
   ],
   "source": [
    "!\"./model_download.sh\" \"$model_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2f521d-89e0-480d-8bda-9ff7a8212843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abaa497d-7634-4e1f-9f71-ebe38b58c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 300, 300, 3)]     0         \n",
      "                                                                 \n",
      " convnext_base (Functional)  (None, 1024)              87566464  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " species (Dense)             (None, 89)                91225     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,657,689\n",
      "Trainable params: 87,657,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f56eba-cb55-4ab1-863f-513f3102adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.yaml\") as f:\n",
    "    labels = yaml.load(f, yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c144993-a9aa-42d0-9852-3d39e94ceb80",
   "metadata": {},
   "source": [
    "### Run Predictions on a single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "746526e8-9820-41b5-8535-9ca75f4d906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_crop_image(metadata, image_shape=[300,300]):\n",
    "    \"\"\"\n",
    "    Loads an image from metadata and crops it to it's bounding box\n",
    "    :param metadata: the image metadata\n",
    "    :return: the cropped image\n",
    "    \"\"\"\n",
    "\n",
    "    img = tf.io.read_file(metadata['file'])\n",
    "    img = tf.io.decode_image(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "\n",
    "    if len(metadata['bbox']) == 0:\n",
    "        bbox = [0.0, 0.0, 1.0, 1.0]\n",
    "    else:\n",
    "        bbox = metadata['bbox']\n",
    "        bbox = [bbox[1], bbox[0], bbox[1]+bbox[3], bbox[0]+bbox[2]]\n",
    "\n",
    "    cropped_img = tf.image.crop_and_resize([img], [bbox], [0], image_shape, method='bilinear')\n",
    "\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41633586-68b6-481a-ae65-785dc849eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_decode(img, model, labels, top_k = None):\n",
    "    \"\"\"\n",
    "    Performs model predictions on a given cropped image and returns the decoded top predictions\n",
    "    :param img: the cropped image to run predictions\n",
    "    :param model: the model to perform predictions with\n",
    "    :param labels: the labels to decode the predictions\n",
    "    :param top_k: if given return the top k predictions instead of the top prediction only\n",
    "    :return: the decoded top predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = model.predict(img)[0]\n",
    "    \n",
    "    if top_k is not None:\n",
    "        top_k_preds = tf.math.top_k(preds, k=top_k)\n",
    "        preds_decoded = [(labels[top_k_preds.indices.numpy()[t]],\n",
    "                                   top_k_preds.values.numpy()[t]) for t in range(top_k)]\n",
    "    else:\n",
    "        preds_decoded = labels[tf.math.argmax(preds).numpy()]\n",
    "    return preds_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2cc2ffc-acf4-489d-b5df-e7fddbdf2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"file\": \"../data/MOF/img/20211118/02/IMAG0080.JPG\",\n",
    "    \"bbox\": [0.0871, 0.4609, 0.3328, 0.4593],\n",
    "    \"conf\": 0.949,\n",
    "}\n",
    "img = load_and_crop_image(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e585a39-caa1-441d-9e2e-2d7e7c79d53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sus_scrofa'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_and_decode(img, model, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
